# NLP-BASICS-PROCEDURES
Natural Language Processing (NLP) Techniques with Example Code

This Jupyter notebook provides a comprehensive guide to various Natural Language Processing (NLP) techniques using Python and popular libraries like NLTK, Spacy, TextBlob, and Hugging Face Transformers. The notebook covers the following NLP techniques, each accompanied by example code:

Text Preprocessing:
Lowercasing: Convert text to lowercase to ensure uniformity.
Removing Punctuations: Eliminate punctuation marks from the text.
Removing Special Characters and Numbers: Remove non-alphanumeric characters.
Removal of HTML Tags: Strip HTML tags from text data.
Removing URLs: Get rid of hyperlinks present in the text.
Removing Extra Spaces: Trim excessive spaces between words.
Expanding Contractions: Expand contracted words like "won't" to "will not".
Text Correction: Perform basic text correction operations.
Tokenization:
Sentence Tokenization: Split the text into individual sentences.
Word Tokenization: Tokenize the text into individual words.
Sub-Word (n-gram character) Tokenization: Create sub-word tokens, useful for some language models.
Stop Word Removal:
Eliminate common stop words from the text.
Stemming:
Porter Stemmer: Apply the Porter stemming algorithm.
Snowball Stemmer: Use the Snowball stemming algorithm.
Lancaster Stemmer: Apply the Lancaster stemming algorithm.
Regexp Stemmer: Perform stemming using regular expressions.
Lemmatization:
Wordnet Lemmatizer: Lemmatize words using WordNet from NLTK.
TextBlob Lemmatizer: Lemmatize using TextBlob.
More Advanced Techniques: Use POS tagging for improved lemmatization.
NER Tagging (Named Entity Recognition):
NER using NLTK: Identify named entities using NLTK.
NER using Spacy: Leverage the Spacy library for named entity recognition.
Text to Numerical Vector Conversion Techniques:
BOW (Bag Of Words): Convert text data into numerical vectors using Count Vectorizer.
TF-IDF (Term Frequency-Inverse Document Frequency): Utilize TF-IDF vectorization.
Word2Vec: Implement Word2Vec for word embedding.
GloVe (Global Vectors): Use GloVe pre-trained word embeddings.
BERT (Bidirectional Encoder Representations from Transformers): Apply BERT for contextual word representations.
Each section in the notebook provides example code to demonstrate the practical implementation of these NLP techniques. By following this guide, you can gain a better understanding of various NLP preprocessing, feature extraction, and representation techniques, and leverage them for your NLP projects.

Happy NLP exploration! ðŸš€
